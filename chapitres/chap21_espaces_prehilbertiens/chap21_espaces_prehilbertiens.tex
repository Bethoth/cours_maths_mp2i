\chapter{Espaces préhilbertiens}

\minitoc

\section{Produit scalaire, norme associée}

\subsection{Produit scalaire}

\begin{defi}[Produit scalaire]
Soit \(E\) un \(\R\)-espace vectoriel.

On appelle produit scalaire sur \(E\) tout forme bilinéaire symétrique définie positive, \cad toute application \[\fonction{\ps{\cdot}{\cdot}}{E\times E}{\R}{\paren{x,y}}{\ps{x}{y}}\] qui vérifie : \[\begin{dcases}
\ps{\cdot}{\cdot}\text{ est bilinéaire : }\begin{dcases}
\quantifs{\forall\lambda,\mu\in\R;\forall x_1,x_2,y\in E}\ps{\lambda x_1+\mu x_2}{y}=\lambda\ps{x_1}{y}+\mu\ps{x_2}{y} \\
\quantifs{\forall\lambda,\mu\in\R;\forall x,y_1,y_2\in E}\ps{x}{\lambda y_1+\mu y_2}=\lambda\ps{x}{y_1}+\mu\ps{x}{y_2}
\end{dcases} \\
\ps{\cdot}{\cdot}\text{ est symétrique : }\quantifs{\forall x,y\in E}\ps{x}{y}=\ps{y}{x} \\
\ps{\cdot}{\cdot}\text{ est définie positive : }\quantifs{\forall x\in E}\begin{dcases}
\ps{x}{x}\geq0 \\
\ps{x}{x}=0\ssi x=0_E
\end{dcases}
\end{dcases}\]

Le produit scalaire de deux vecteurs \(x\) et \(y\) est traditionnellement noté \(\ps{x}{y}\), \(\left(x\tq y\right)\), \(\left\langle x,y\right\rangle\) ou \(x\cdot y\). On le notera \(\ps{x}{y}\) dans tout ce cours.
\end{defi}

\begin{defi}[Espace euclidien]
On appelle espace préhilbertien (réel) tout \(\R\)-espace vectoriel muni d'un produit scalaire.

On appelle espace euclidien tout \(\R\)-espace vectoriel de dimension finie muni d'un produit scalaire.
\end{defi}

\begin{ex}\thlabel{ex:produitsScalaires}
Les exemples suivants sont à connaître parfaitement :

\begin{enumerate}
\item Soit \(n\in\Ns\). L'application : \[\fonctionlambda{\R^n\times\R^n}{\R}{\paren{\tcoords{x_1}{\vdots}{x_n},\tcoords{x_1\prim}{\vdots}{x_n\prim}}}{x_1x_1\prim+\dots+x_nx_n\prim}\] est un produit scalaire sur \(\R^n\) appelé le produit scalaire canonique de \(\R^n\).

Ce produit scalaire s'écrit aussi : \[\fonctionlambda{\R^n\times\R^n}{\R}{\paren{X,X\prim}}{\trans{X}X\prim}\]

\item Soient \(a,b\in\R\) tels que \(a<b\). On pose \(E=\ensclasse{0}{\intervii{a}{b}}{\R}\). L'application : \[\fonctionlambda{E\times E}{\R}{\paren{f,g}}{\int_a^bf\paren{t}g\paren{t}\odif{t}}\] est un produit scalaire sur \(E\). \\

\item L'application : \[\fonctionlambda{\M{n}[\R]\times\M{n}[\R]}{\R}{\paren{A,B}}{\tr\paren{\trans{A}B}}\] est un produit scalaire sur \(\M{n}[\R]\) appelé le produit scalaire canonique de \(\M{n}[\R]\).

On a, pour toutes matrices \(A=\paren{a_{ij}}_{\paren{i,j}\in\interventierii{1}{n}^2}\) et \(B=\paren{b_{ij}}_{\paren{i,j}\in\interventierii{1}{n}^2}\) : \[\tr\paren{\trans{A}B}=\sum_{i=1}^n\sum_{j=1}^na_{ij}b_{ij}.\]

\item Plus généralement, l'application : \[\fonctionlambda{\M{np}[\R]\times\M{np}[\R]}{\R}{\paren{A,B}}{\tr\paren{\trans{A}B}}\] est un produit scalaire sur \(\M{np}[\R]\) appelé le produit scalaire canonique de \(\M{np}[\R]\).

On a, pour toutes matrices \(A=\paren{a_{ij}}_{\paren{i,j}\in\interventierii{1}{n}\times\interventierii{1}{p}}\) et \(B=\paren{b_{ij}}_{\paren{i,j}\in\interventierii{1}{n}\times\interventierii{1}{p}}\) : \[\tr\paren{\trans{A}B}=\sum_{i=1}^n\sum_{j=1}^pa_{ij}b_{ij}.\]
\end{enumerate}
\end{ex}

\begin{dem}[2]\thlabel{dem:produitScalaireFonctionsContinues}
On note \(\phi:E^2\to\R\) l'application.

On a : \[\quantifs{\forall f,g\in E}\phi\paren{f,g}=\phi\paren{g,f}.\]

Donc \(\phi\) est symétrique.

On a : \[\begin{aligned}
\quantifs{\forall\lambda,\mu\in\R;\forall f_1,f_2,g\in E}\phi\paren{\lambda f_1+\mu f_2,g}&=\int_a^b\paren{\lambda f_1\paren{t}+\mu f_2\paren{t}}g\paren{t}\odif{t} \\
&=\lambda\int_a^bf_1\paren{t}g\paren{t}\odif{t}+\mu\int_a^bf_2\paren{t}g\paren{t}\odif{t} \\
&=\lambda\phi\paren{f_1,g}+\mu\phi\paren{f_2,g}.
\end{aligned}\]

Donc \(\phi\) est linéaire à gauche.

Comme \(\phi\) est symétrique, \(\phi\) est aussi linéaire à droite.

Donc \(\phi\) est bilinéaire.

Soit \(f\in E\).

On a : \[\phi\paren{f,f}=\int_a^bf^2\paren{t}\odif{t}\geq0.\]

Donc \(\phi\) est définie positive.

Enfin, si \(\phi\paren{f,f}=0\) alors \(\int_a^bf^2\paren{t}\odif{t}=0\).

Or la fonction \(f^2\) est continue et positive sur \(\intervii{a}{b}\) donc \(f^2=0\) donc \(f=0\).

Finalement, \(\phi\) est un produit scalaire sur \(E\).
\end{dem}

\begin{dem}[Autres exemples]
\note{Exercice}
\end{dem}

\subsection{Norme associée à un produit scalaire}

\begin{nota}
Soit \(E\) un espace préhilbertien.

On pose : \[\quantifs{\forall x\in E}\norme{x}=\sqrt{\ps{x}{x}}.\]

On étudie dans la suite l'application : \[\fonction{\norme{\cdot}}{E}{\R}{x}{\norme{x}=\sqrt{\ps{x}{x}}}\] appelée norme associée au produit scalaire de \(E\).
\end{nota}

\begin{theo}[Inégalité de Cauchy-Schwarz]
Soient \(E\) un espace préhilbertien et \(x,y\in E\).

On a l'inégalité de Cauchy-Schwarz : \[\abs{\ps{x}{y}}\leq\norme{x}\norme{y}.\]

De plus, on a les cas d'égalités dans l'inégalité de Cauchy-Schwarz : \[\abs{\ps{x}{y}}=\norme{x}\norme{y}\ssi x\text{ et }y\text{ sont colinéaires}\] et : \[\ps{x}{y}=\norme{x}\norme{y}\ssi\quantifs{\exists\lambda\in\Rp}\orenv{x=\lambda y \\ y=\lambda x}\]
\end{theo}

\begin{dem}[Inégalité de Cauchy-Schwarz]
Si \(y=0_E\) alors \(\ps{x}{y}=\norme{x}\norme{y}=0\).

Supposons \(y\not=0_E\).

On pose \(\fonction{f}{\R}{\R}{\lambda}{\norme{x+\lambda y}^2}\)

On a : \[\begin{WithArrows}
\quantifs{\forall\lambda\in\R}f\paren{\lambda}&=\ps{x+\lambda y}{x+\lambda y} \Arrow{car \(\ps{\cdot}{\cdot}\) est bilinéaire} \\
&=\ps{x}{x}+\lambda\ps{x}{y}+\lambda^2\ps{y}{x}+\ps{y}{y} \Arrow{car \(\ps{\cdot}{\cdot}\) est symétrique} \\
&=\lambda^2\underbrace{\norme{y}^2}_{\not=0}+2\lambda\ps{x}{y}+\norme{x}^2 \Arrow{car \(\ps{\cdot}{\cdot}\) est positif} \\
&\geq0.
\end{WithArrows}\]

Donc \(f\) est une fonction polynomiale de degré \(2\) positive sur \(\R\).

Donc le discriminant de \(\norme{y}^2X^2+2\ps{x}{y}X+\norme{x}^2\) est négatif : \[\Delta=4\ps{x}{y}^2-4\norme{x}^2\norme{y}^2\leq0.\]

D'où \(\abs{\ps{x}{y}}\leq\norme{x}\norme{y}\).
\end{dem}

\begin{dem}[Cas d'égalité en valeur absolue]
Si \(y=0_E\), l'équivalence est vraie.

Supposons \(y=0_E\).

On a : \[\begin{WithArrows}
\abs{\ps{x}{y}}=\norme{x}\norme{y}&\ssi\Delta=0 \\
&\ssi\quantifs{\exists\lambda\in\R}f\paren{\lambda}=0 \\
&\ssi\quantifs{\exists\lambda\in\R}\norme{x+\lambda y}^2=0 \\
&\ssi\quantifs{\exists\lambda\in\R}x=-\lambda y \Arrow{car \(y\not=0_E\)} \\
&\ssi x\text{ et }y\text{ sont colinéaires}.
\end{WithArrows}\]
\end{dem}

\begin{dem}[Cas d'égalité]
On a : \[\begin{aligned}
\ps{x}{y}=\norme{x}\norme{y}&\ssi\begin{dcases}
x\text{ et }y\text{ sont colinéaires} \\
\ps{x}{y}=\norme{x}\norme{y}
\end{dcases} \\
&\ssi\begin{dcases}
\quantifs{\exists\lambda\in\R}\orenv{x=\lambda y \\ y=\lambda x} \\
\ps{x}{y}=\norme{x}\norme{y}
\end{dcases} \\
&\ssi\quantifs{\exists\lambda\in\Rp}\orenv{x=\lambda y \\ y=\lambda x}
\end{aligned}\]
\end{dem}

\begin{ex}
Soient \(a,b\in\R\) tels que \(a<b\) et \(f,g\in\ensclasse{0}{\intervii{a}{b}}{\R}\).

On a : \[\abs{\int_a^bf\paren{t}g\paren{t}\odif{t}}\leq\sqrt{\int_a^bf^2\paren{t}\odif{t}}\sqrt{\int_a^bg^2\paren{t}\odif{t}}.\]
\end{ex}

\begin{dem}
C'est l'inégalité de Cauchy-Schwarz appliquée au produit scalaire (2) de l'\thref{ex:produitsScalaires}.
\end{dem}

\begin{theo}[Inégalité de Minkowski ou inégalité triangulaire pour la norme]
Soient \(E\) un espace préhilbertien et \(x,y\in E\).

On a l'inégalité de Minkowski : \[\norme{x+y}\leq\norme{x}+\norme{y}.\]

De plus, on a le cas d'égalité dans l'inégalité de Minkowski : \[\norme{x+y}=\norme{x}+\norme{y}\ssi\quantifs{\exists\lambda\in\Rp}\orenv{x=\lambda y \\ y=\lambda x}\]
\end{theo}

\begin{dem}[Inégalité de Minkowski]
On a : \[\begin{aligned}
\norme{x+y}\leq\norme{x}+\norme{y}&\ssi\norme{x+y}^2\leq\paren{\norme{x}+\norme{y}}^2 \\
&\ssi\ps{x+y}{x+y}\leq\paren{\norme{x}+\norme{y}}^2 \\
&\ssi\norme{x}^2+2\ps{x}{y}+\norme{y}^2\leq\norme{x}^2+2\norme{x}\norme{y}+\norme{y}^2 \\
&\color{white}\ssi\color{black}\text{ce qui est vrai selon l'inégalité de Cauchy-Schwarz}.
\end{aligned}\]
\end{dem}

\begin{dem}[Cas d'égalité]
Découle du cas d'égalité sans valeur absolue dans l'inégalité de Cauchy-Schwarz.
\end{dem}

\begin{ex}
Soient \(a,b\in\R\) tels que \(a<b\) et \(f,g\in\ensclasse{0}{\intervii{a}{b}}{\R}\).

On a : \[\sqrt{\int_a^b\paren{f\paren{t}+g\paren{t}}^2\odif{t}}\leq\sqrt{\int_a^bf^2\paren{t}\odif{t}}+\sqrt{\int_a^bg^2\paren{t}\odif{t}}.\]
\end{ex}

\begin{dem}
C'est l'inégalité de Minkowski appliquée au produit scalaire (2) de l'\thref{ex:produitsScalaires}.
\end{dem}

\begin{rem}
Soit \(E\) un espace préhilbertien.

La norme \(\norme{\cdot}\) associée au produit scalaire de \(E\) vérifie : \[\begin{dcases}
\quantifs{\forall x\in E}\norme{x}=0\imp x=0_E \\
\quantifs{\forall\lambda\in\R;\forall x\in E}\norme{\lambda x}=\abs{\lambda}\norme{x} \\
\quantifs{\forall x,y\in E}\norme{x+y}\leq\norme{x}+\norme{y}
\end{dcases}\]

En deuxième année, vous étudierez les fonction \(E\to\Rp\) vérifiant ces trois propriétés. Une telle fonction sur un \(\R\) (ou \(\C\))-espace vectoriel est appelée une \guillemets{norme}.

Ainsi, dans ce paragraphe, on a montré que la \guillemets{norme associée à un produit scalaire} est bien ce qu'on appelle une \guillemets{norme}.
\end{rem}

\begin{dem}
Soit \(x\in E\).

On a : \[\begin{aligned}
\norme{x}=0&\ssi\sqrt{\ps{x}{x}}=0 \\
&\ssi\ps{x}{x}=0 \\
&\ssi x=0.
\end{aligned}\]

Soit \(\lambda\in\R\).

On a : \[\begin{aligned}
\norme{\lambda x}&=\sqrt{\ps{\lambda x}{\lambda x}} \\
&=\sqrt{\lambda^2\ps{x}{x}} \\
&=\abs{\lambda}\norme{x}.
\end{aligned}\]
\end{dem}

\begin{defprop}[Distance]
Soit \(E\) un espace préhilbertien.

On appelle distance associée au produit scalaire de \(E\) l'application : \[\fonction{d}{E\times E}{\Rp}{\paren{x,y}}{\norme{y-x}}\]

Elle vérifie : \[\begin{dcases}
\quantifs{\forall x,y\in E}d\paren{x,y}=0\ssi x=y \\
\quantifs{\forall x,y\in E}d\paren{x,y}=d\paren{y,x} \\
\quantifs{\forall x,y,z\in E}d\paren{x,z}\leq d\paren{x,y}+d\paren{y,z}
\end{dcases}\]
\end{defprop}

\subsection{Propriétés}

Soit \(E\) un espace préhilbertien dont on note \(\ps{\cdot}{\cdot}\) le produit scalaire et \(\norme{\cdot}\) la norme associée.

\begin{prop}\thlabel{prop:carréDeLaNormeD'UneSommeOuD'uneDifférence}
Soient \(x,y\in E\).

On a : \[\norme{x+y}^2=\norme{x}^2+2\ps{x}{y}+\norme{y}^2\qquad\text{et}\qquad\norme{x-y}^2=\norme{x}^2-2\ps{x}{y}+\norme{y}^2.\]
\end{prop}

\begin{dem}
On a : \[\norme{x+y}^2=\ps{x+y}{x+y}=\ps{x}{x}+\ps{x}{y}+\ps{y}{x}+\ps{y}{y}=\norme{x}^2+2\ps{x}{y}+\norme{y}^2\] et : \[\norme{x-y}^2=\ps{x-y}{x-y}=\ps{x}{x}-\ps{x}{y}-\ps{y}{x}+\ps{y}{y}=\norme{x}^2-2\ps{x}{y}+\norme{y}^2.\]
\end{dem}

\begin{theo}[Identité du parallélogramme]
Soient \(x,y\in E\).

On a : \[\norme{x+y}^2+\norme{x-y}^2=2\norme{x}^2+2\norme{y}^2.\]
\end{theo}

\begin{dem}
Découle de la \thref{prop:carréDeLaNormeD'UneSommeOuD'uneDifférence} par somme des deux égalités.
\end{dem}

\begin{theo}[Identités de polarisation]
Soient \(x,y\in E\).

On a : \[\begin{dcases}
\ps{x}{y}=\dfrac{1}{2}\paren{\norme{x+y}^2-\norme{x}^2-\norme{y}^2} \\
\ps{x}{y}=\dfrac{1}{4}\paren{\norme{x+y}^2-\norme{x-y}^2}
\end{dcases}\]
\end{theo}

\begin{dem}
Découle de la \thref{prop:carréDeLaNormeD'UneSommeOuD'uneDifférence}.
\end{dem}

\section{Orthogonalité, base orthonormale}

\subsection{Vocabulaire}

\begin{defi}
Soit \(E\) un espace préhilbertien.

On dit qu'un vecteur \(x\in E\) est unitaire s'il est de norme \(1\) : \[\norme{x}=1.\]

On dit que deux vecteurs \(x,y\in E\) sont orthogonaux et on note \(x\perp y\) si leur produit scalaire est nul : \[\ps{x}{y}=0.\]

On dit qu'une famille de vecteurs \(\paren{x_i}_{i\in I}\in E^I\) est orthogonale si ses vecteurs sont deux à deux orthogonaux : \[\quantifs{\forall i,j\in I}i\not=j\imp\ps{x_i}{x_j}=0.\]

On dit qu'une famille de vecteurs \(\paren{x_i}_{i\in I}\in E^I\) est orthonormale ou orthonormée si ses vecteurs sont unitaires et deux à deux orthogonaux : \[\quantifs{\forall i,j\in I}\ps{x_i}{x_j}=\delta_{ij}.\]

On dit qu'une famille de vecteurs est une base orthonormale ou orthonormée de \(E\) si c'est une famille orthonormale et une base de \(E\).
\end{defi}

\begin{ex}
La base canonique de \(\R^n\), pour le produit scalaire canonique de \(\R^n\), est orthonormale.

La base canonique de \(\M{np}[\R]\), pour le produit scalaire canonique de \(\M{np}[\R]\), est orthonormale.
\end{ex}

\begin{exo}
On note \(\fami{C}_{2\pi}\) l'ensemble des fonctions continues \(2\pi\)-périodiques de \(\R\) dans \(\R\).

\begin{enumerate}
\item Montrer que l'application : \[\fonctionlambda{\fami{C}_{2\pi}\times\fami{C}_{2\pi}}{\R}{\paren{f,g}}{\dfrac{1}{2\pi}\int_0^{2\pi}f\paren{t}g\paren{t}\odif{t}}\] est un produit scalaire. L'espace vectoriel \(\fami{C}_{2\pi}\) est-il un espace euclidien ? \\
\item On pose : \[\quantifs{\forall k\in\interventierii{0}{n}}\fonction{f_k}{\R}{\R}{t}{\cos\paren{kt}}\] La famille \(\paren{f_0,\dots,f_n}\) est-elle orthogonale ? orthonormale ? \\
\item Même question avec la famille \(\paren{f_0,\dots,f_n,g_1,\dots,g_n}\), en posant : \[\quantifs{\forall k\in\interventierii{1}{n}}\fonction{g_k}{\R}{\R}{t}{\sin\paren{kt}}\]
\end{enumerate}
\end{exo}

\begin{corr}[1]
\Cf \thref{dem:produitScalaireFonctionsContinues}.

On note tout de même :

Soit \(f\in\fami{C}_{2\pi}\).

Si \(\ps{f}{f}=0\) alors \(\dfrac{1}{2\pi}\int_0^{2\pi}f^2\paren{t}\odif{t}=0\).

Or \(f^2\) est continue et positive.

Donc \(\quantifs{\forall t\in\intervii{0}{2\pi}}f^2\paren{t}=0\).

Donc \(f=0\) car \(f\) est \(2\pi\)-périodique.
\end{corr}

\begin{corr}[2]
On a : \[\begin{aligned}
\quantifs{\forall k,l\in\interventierii{0}{n}}\ps{f_k}{f_l}&=\dfrac{1}{2\pi}\int_0^{2\pi}f_k\paren{t}f_l\paren{t}\odif{t} \\
&=\dfrac{1}{2\pi}\int_0^{2\pi}\cos\paren{kt}\cos\paren{lt}\odif{t} \\
&=\dfrac{1}{4\pi}\paren{\int_0^{2\pi}\cos\paren{\paren{k+l}t}\odif{t}+\int_0^{2\pi}\cos\paren{\paren{k-l}t}\odif{t}} \\
&=\begin{dcases}
\dfrac{1}{4\pi}\paren{\croch{\dfrac{\sin\paren{\paren{k+l}t}}{k+l}}_0^{2\pi}+\croch{\dfrac{\sin\paren{\paren{k-l}t}}{k-l}}_0^{2\pi}}=0 &\text{si }k\not=l \\
\dfrac{1}{4\pi}\paren{\croch{\dfrac{\sin\paren{2kt}}{2k}}_0^{2\pi}+2\pi}=\dfrac{1}{2}\not=1 &\text{si }k=l\not=0 \\
\dfrac{1}{4\pi}\paren{2\pi+2\pi}=1 &\text{si }k=l=0
\end{dcases}
\end{aligned}\]

Donc \(\paren{f_0,\dots,f_n}\) est orthogonale mais pas orthonormée car \(\quantifs{\forall k\in\interventierii{1}{n}}\norme{f_k}=\dfrac{1}{\sqrt{2}}\not=1\).
\end{corr}

\begin{corr}[3]
On a : \[\begin{aligned}
\quantifs{\forall k\in\interventierii{0}{n};\forall l\in\interventierii{1}{n}}\ps{f_k}{g_l}&=\dfrac{1}{2\pi}\int_0^{2\pi}f_k\paren{t}g_l\paren{t}\odif{t} \\
&=\dfrac{1}{2\pi}\int_0^{2\pi}\cos\paren{kt}\sin\paren{lt}\odif{t} \\
&=\dfrac{1}{4\pi}\paren{\int_0^{2\pi}\sin\paren{\paren{k+l}t}\odif{t}+\int_0^{2\pi}\sin\paren{\paren{l-k}t}\odif{t}} \\
&=\begin{dcases}
\dfrac{1}{4\pi}\paren{\croch{\dfrac{-\cos\paren{\paren{k+l}t}}{k+l}}_0^{2\pi}+\croch{\dfrac{-\cos\paren{\paren{l-k}t}}{l-k}}_0^{2\pi}}=0 &\text{si }k\not=l \\
\dfrac{1}{4\pi}\paren{\croch{\dfrac{-\cos\paren{2kt}}{2k}}_0^{2\pi}+0}=0 &\text{si }k=l
\end{dcases}
\end{aligned}\]

Donc \(\quantifs{\forall k\in\interventierii{0}{n};\forall l\in\interventierii{1}{n}}f_k\perp g_l\).

De plus, on a : \[\begin{aligned}
\quantifs{\forall k,l\in\interventierii{1}{n}}\ps{g_k}{g_l}&=\dfrac{1}{2\pi}\int_0^{2\pi}g_k\paren{t}g_l\paren{t}\odif{t} \\
&=\dfrac{1}{2\pi}\int_0^{2\pi}\sin\paren{kt}\sin\paren{lt}\odif{t} \\
&=\dfrac{1}{4\pi}\paren{\int_0^{2\pi}\cos\paren{\paren{k-l}t}\odif{t}-\int_0^{2\pi}\cos\paren{\paren{k+l}t}\odif{t}} \\
&=\begin{dcases}
\dfrac{1}{4\pi}\paren{\croch{\dfrac{\sin\paren{\paren{k-l}t}}{k-l}}_0^{2\pi}-\croch{\dfrac{\sin\paren{\paren{k+l}t}}{k+l}}_0^{2\pi}}=0 &\text{si }k\not=l \\
\dfrac{1}{4\pi}\paren{-\croch{\dfrac{\sin\paren{2kt}}{2k}}_0^{2\pi}+2\pi}=\dfrac{1}{2} &\text{si }k=l
\end{dcases}
\end{aligned}\]

Donc \(\quantifs{\forall k,l\in\interventierii{1}{n}}g_k\perp g_l\).

Finalement, \(\paren{f_0,\dots,f_n,g_1,\dots,g_n}\) est orthogonale mais pas orthonormée car \(\quantifs{\forall k\in\interventierii{1}{n}}\norme{g_k}=\dfrac{1}{\sqrt{2}}\not=1\).
\end{corr}

\subsection{Propriétés des familles orthogonales}

\begin{theo}[Théorème de Pythagore]
Soient \(E\) un espace préhilbertien et \(\paren{x_1,\dots,x_n}\) une famille orthogonale de vecteurs de \(E\).

On a : \[\norme{\sum_{j=1}^{n}x_j}^2=\sum_{j=1}^{n}\norme{x_j}^2.\]
\end{theo}

\begin{dem}
On a : \[\begin{aligned}
\norme{\sum_{j=1}^{n}x_j}^2&=\ps{\sum_{j=1}^{n}x_j}{\sum_{k=1}^{n}x_k} \\
&=\sum_{j=1}^{n}\sum_{k=1}^{n}\underbrace{\ps{x_j}{x_k}}_{=0\text{ si }j\not=k} \\
&=\sum_{j=1}^{n}\norme{x_j}^2
\end{aligned}\]
\end{dem}

\begin{prop}
Soient \(E\) un espace préhilbertien et \(\paren{x_1,\dots,x_n}\in E^n\) une famille orthogonale dont tous les vecteurs sont non-nuls.

Alors \(\paren{x_1,\dots,x_n}\) est une famille libre.
\end{prop}

\begin{dem}
Soient \(\lambda_1,\dots,\lambda_n\in\R\) tels que \(\sum_{j=1}^{n}\lambda_jx_j=0_E\) et \(k\in\interventierii{1}{n}\).

On a \(\ps{\sum_{j=1}^{n}\lambda_jx_j}{x_k}=0\).

Donc \(\sum_{j=1}^{n}\lambda_j\underbrace{\ps{x_j}{x_k}}_{=0\text{ si }j\not=k}=0\).

Donc \(\lambda_k\norme{x_k}^2=0\).

Donc \(\lambda_k=0\) car \(x_k\not=0\).

Donc \(\paren{x_1,\dots,x_n}\) est libre.
\end{dem}

\begin{prop}
Soient \(E\) un espace euclidien de dimension \(n\in\Ns\) et \(\paren{x_1,\dots,x_n}\in E^n\) une famille orthonormale possédant \(n\) vecteurs.

Alors \(\paren{x_1,\dots,x_n}\) est une base orthonormale de \(E\).
\end{prop}

\begin{dem}
La famille \(\paren{x_1,\dots,x_n}\) est libre car c'est une famille orthogonale de vecteurs non-nuls. De plus, elle possède \(n\) vecteurs et \(\dim E=n\) donc c'est une base de \(E\) et donc une base orthonormale de \(E\).
\end{dem}

\subsection{Calculs dans une base orthonormale}

\begin{prop}
Soient \(E\) un espace euclidien de dimension \(n\in\Ns\), \(\fami{B}=\paren{e_1,\dots,e_n}\) une base orthonormale de \(E\) et \(x\) et \(y\) deux vecteurs de \(E\) dont on note \(X=\tcoords{x_1}{\vdots}{x_n}\) et \(Y=\tcoords{y_1}{\vdots}{y_n}\) les coordonnées respectives dans \(\fami{B}\).

On a :

\begin{enumerate}
    \item \(\quantifs{\forall i\in\interventierii{1}{n}}x_i=\ps{e_i}{x}\) ; \\
    \item \(\ps{x}{y}=\sum_{i=1}^{n}x_iy_i=\trans{X}Y\) ; \\
    \item \(\norme{x}=\sqrt{\sum_{i=1}^{n}x_i^2}=\sqrt{\trans{X}X}\).
\end{enumerate}
\end{prop}

\begin{dem}
Tout découle de : \[\begin{dcases}
x=\sum_{i=1}^{n}x_ie_i \\
y=\sum_{i=1}^{n}y_ie_i \\
\quantifs{\forall i,j\in\interventierii{1}{n}}\ps{e_i}{e_j}=\delta_{ij}
\end{dcases}\]

On a : \[\begin{aligned}
\quantifs{\forall i\in\interventierii{1}{n}}\ps{e_i}{x}&=\ps{e_i}{\sum_{j=1}^{n}x_je_j} \\
&=\sum_{j=1}^{n}x_j\ps{e_i}{e_j} \\
&=x_i
\end{aligned}\] et : \[\begin{aligned}
\ps{x}{y}&=\ps{\sum_{i=1}^{n}x_ie_i}{\sum_{j=1}^{n}y_je_j} \\
&=\sum_{i=1}^{n}\sum_{j=1}^{n}x_iy_j\ps{e_i}{e_j} \\
&=\sum_{i=1}^{n}x_iy_i.
\end{aligned}\]
\end{dem}

\begin{rem}
Attention au fait que les formules précédentes ne sont valables que dans une base orthonormale.

Ainsi, si \(\paren{e_1,\dots,e_n}\) est une base orthonormale de \(E\), alors on a : \[\quantifs{\forall x\in E}x=\sum_{i=1}^{n}\ps{e_i}{x}e_i.\]
\end{rem}

\begin{cor}
Soient \(E\) un espace euclidien, \(\fami{B}=\paren{e_1,\dots,e_n}\) une base orthonormale de \(E\) et \(u\in\Lendo{E}\).

La matrice de \(u\) dans \(\fami{B}\) est : \[\Mat{u}=\begin{pmatrix}
\ps{e_1}{u\paren{e_1}} & \dots & \ps{e_1}{u\paren{e_n}} \\
\vdots &  & \vdots \\
\ps{e_n}{u\paren{e_1}} & \dots & \ps{e_n}{u\paren{e_n}}
\end{pmatrix}.\]
\end{cor}

\section{Sous-espaces vectoriels}

\subsection{Sous-espaces vectoriels orthogonaux}

\begin{defi}
Soient \(E\) un espace préhilbertien et \(F\) et \(G\) deux sous-espaces vectoriels de \(E\).

On dit que \(F\) et \(G\) sont deux sous-espaces vectoriels orthogonaux et on note \(F\perp G\) si on a : \[\quantifs{\forall x\in F;\forall y\in G}x\perp y.\]
\end{defi}

\begin{ex}
Soit \(E\) un espace préhilbertien.

Soient \(x,y\in E\) tels que \(x\perp y\). On a alors \(\Vect{x}\perp\Vect{y}\).

Le sous-espace vectoriel nul \(\accol{0_E}\) est orthogonal à tous les sous-espaces vectoriels de \(E\) (car le vecteur nul de \(E\) est orthogonal à tous les vecteurs de \(E\)).
\end{ex}

\begin{defi}[Orthogonal d'une partie]
Soient \(E\) un espace préhilbertien et \(A\subset E\).

On appelle orthogonal de \(A\) (dans \(E\)) et on note \(A\ortho\) l'ensemble des vecteurs de \(E\) qui sont orthogonaux à tous les vecteurs de \(A\) : \[A\ortho=\accol{x\in E\tq\quantifs{\forall y\in A}x\perp y}.\]

L'ensemble \(A\ortho\) est un sous-espace vectoriel de \(E\).
\end{defi}

\begin{prop}[Orthogonal d'un sous-espace vectoriel]\thlabel{prop:orthogonalD'UnSousEspaceVectoriel}
Soient \(E\) un espace préhilbertien et \(F\) un sous-espace vectoriel de \(E\).

On a :

\begin{enumerate}
    \item \(F\ortho\) est un sous-espace vectoriel de \(E\) ; \\
    \item \(F\perp F\ortho\) ; \\
    \item \(F\inter F\ortho=\accol{0_E}\) ; \\
    \item \(F\subset\paren{F\ortho}\ortho\).
\end{enumerate}
\end{prop}

\begin{dem}[1]
On a \(F\ortho\subset E\) et \(0_E\in F\ortho\) car \(\quantifs{\forall y\in F}0_E\perp y\).

Soient \(\lambda_1,\lambda_2\in\R\) et \(x_1,x_2\in F\ortho\).

On a : \[\quantifs{\forall y\in F}\ps{\lambda_1x_1+\lambda_2x_2}{y}=\lambda_1\underbrace{\ps{x_1}{y}}_{=0}+\lambda_2\underbrace{\ps{x_2}{y}}_{=0}=0.\]

Donc \(\lambda_1x_1+\lambda_2x_2\in F\ortho\).

Donc \(F\ortho\) est un sous-espace vectoriel de \(E\).
\end{dem}

\begin{dem}[2]
Clair par définition de \(F\ortho\).
\end{dem}

\begin{dem}[3]
Soit \(x\in F\inter F\ortho\).

On a \(x\perp x\) donc \(\ps{x}{x}=0\).

Donc \(x=0_E\).
\end{dem}

\begin{dem}[4]
Soit \(x\in F\).

On a \(\quantifs{\forall y\in F\ortho}x\perp y\).

Donc \(x\in\paren{F\ortho}\ortho\).
\end{dem}

\begin{rem}
Soient \(E\) un espace préhilbertien et \(F\) et \(G\) deux sous-espaces vectoriels de \(E\).

Les propositions suivantes sont équivalentes :

\begin{enumerate}
    \item \(F\perp G\) \\
    \item \(F\subset G\ortho\) \\
    \item \(G\subset F\ortho\)
\end{enumerate}
\end{rem}

\begin{dem}
On a : \[\begin{aligned}
F\perp G&\ssi\quantifs{\forall x\in F;\forall y\in G}x\perp y \\
&\ssi\quantifs{\forall x\in F}x\in G\ortho \\
&\ssi F\subset G\ortho.
\end{aligned}\]

D'où (1) \(\ssi\) (2).

Idem pour (1) \(\ssi\) (3).
\end{dem}

\subsection{Supplémentaire orthogonal}

\begin{rappel}
Soient \(E\) un espace préhilbertien et \(F\) un sous-espace vectoriel de \(E\).

Selon la \thref{prop:orthogonalD'UnSousEspaceVectoriel}, on a : \[F\inter F\ortho=\accol{0_E}.\]

Donc \(F\) et \(F\ortho\) sont en somme directe.
\end{rappel}

\begin{defi}[Supplémentaire orthogonal]
Soient \(E\) un espace préhilbertien et \(F\) un sous-espace vectoriel de \(E\).

Un supplémentaire orthogonal de \(F\) (dans \(E\)) est un supplémentaire de \(F\) (dans \(E\)) qui est orthogonal à \(F\).

En d'autres termes, c'est un sous-espace vectoriel \(G\) de \(E\) tel que : \[\begin{dcases}
F\oplus G=E \\
F\perp G
\end{dcases}\]

On résume parfois ce système avec la notation \(F\operp G=E\) ou des variantes de cette notation.
\end{defi}

\begin{prop}[Unicité du supplémentaire orthogonal]\thlabel{prop:unicitéDuSupplémentaireOrthogonal}
Soient \(E\) un espace préhilbertien et \(F\) un sous-espace vectoriel de \(E\).

Si \(G\) est un supplémentaire orthogonal de \(F\) alors \(G=F\ortho\).
\end{prop}

\begin{dem}
\incdir Claire car \(G\perp F\).

\increc

Soit \(x\in F\ortho\).

Montrons que \(x\in G\).

Soient \(x_F\in F\) et \(x_G\in G\) tels que \(x=x_F+x_G\).

On a \(\ps{x_F}{x_F+x_G}=\ps{x_F}{x}\) donc \[\norme{x_F}^2+\underbrace{\ps{x_F}{x_G}}_{=0\text{ car }F\perp G}=\underbrace{\ps{x_F}{x}}_{=0\text{ car }F\perp G}.\]

Donc \(x_F=0_E\).

Donc \(x=x_G\).

Donc \(x\in G\).
\end{dem}

\begin{rem}
Soient \(E\) un espace préhilbertien et \(F\) un sous-espace vectoriel de \(E\).

D'après la proposition précédente, \(F\) admet un supplémentaire orthogonal si, et seulement si, \(F\ortho\) est un supplémentaire de \(F\) : \(F+F\ortho=E\).
\end{rem}

\begin{exo}\thlabel{exo:orthogonalD'OrthogonalEgalALuiMême}
Soient \(E\) un espace préhilbertien et \(F\) un sous-espace vectoriel de \(E\).

On suppose que \(F\) admet un supplémentaire orthogonal dans \(E\).

Montrer : \[\paren{F\ortho}\ortho=F.\]
\end{exo}

\begin{corr}
On a \(E=F\oplus F\ortho\) donc \(F\) est un supplémentaire orthogonal de \(F\ortho\).

Donc \(F=\paren{F\ortho}\ortho\) selon la \thref{prop:unicitéDuSupplémentaireOrthogonal}.
\end{corr}

\begin{defi}
Soit \(E\) un espace préhilbertien.

Un projecteur \(p\in\Lendo{E}\) est dit orthogonal si son image et son noyau sont orthogonaux : \[\Im p\perp\ker p.\]
\end{defi}

\begin{rem}
Soient \(E\) un espace préhilbertien et \(F\) et \(G\) deux sous-espaces vectoriels supplémentaires dans \(E\).

\begin{enumerate}
    \item Notons \(p\) le projecteur sur \(F\), parallèlement à \(G\). \\ Si \(p\) est un projecteur orthogonal, alors \(G=F\ortho\). \\ On dit simplement que \(p\) est le projecteur orthogonal sur \(F\). \\
    \item Le \guillemets{projecteur orthogonal sur \(F\)} est bien défini si, et seulement si, \(F\) admet un supplémentaire orthogonal.
\end{enumerate}
\end{rem}

\begin{dem}[1]
Si \(p\) est un projecteur orthogonal alors \(F\perp G\) et \(F\oplus G=E\) donc \(G=F\ortho\).
\end{dem}

\begin{rem}
On sait que tout projecteur est le projecteur sur son image, parallèlement à son noyau.

De même, tout projecteur orthogonal \(p\) est le projecteur orthogonal sur son image, \cad le projecteur sur \(\Im p\), parallèlement à \(\paren{\Im p}\ortho\).
\end{rem}

\subsection{Projecteur orthogonal sur un sous-espace vectoriel de dimension finie}

\begin{deftheo}
Soient \(E\) un espace préhilbertien et \(F\) un sous-espace vectoriel de \(E\).

On suppose que \(F\) est de dimension finie.

Alors \(F\) et son orthogonal \(F\ortho\) sont supplémentaires dans \(E\) : \[E=F\oplus F\ortho.\]

Soit \(\paren{e_1,\dots,e_n}\) une base orthonormale de \(F\).

Le projecteur orthogonal \(p_F\) sur \(F\) est donné par : \[\quantifs{\forall x\in E}p_F\paren{x}=\sum_{k=1}^{n}\ps{e_k}{x}e_k.\]
\end{deftheo}

\begin{dem}
Posons \(\fonction{q}{E}{F}{x}{\sum_{k=1}^{n}\ps{e_k}{x}e_k}\)

On a bien \(q\in\L{E}{F}\).

On remarque : \(\quantifs{\forall x\in E}x=\underbrace{q\paren{x}}_{\in F}+\underbrace{x-q\paren{x}}_{\in F\ortho\text{ ?}}\).

Soit \(x\in E\). Il suffit de montrer que \(x-q\paren{x}\in F\ortho\).

En effet, on en déduira que \(E=F+F\ortho\) donc \(E=F\oplus F\ortho\) et que le projecteur orthogonal sur \(F\) est \(q\).

On a : \[\begin{aligned}
\quantifs{\forall k\in\interventierii{1}{n}}\ps{e_k}{x-q\paren{x}}&=\ps{e_k}{x}-\ps{e_k}{\sum_{l=1}^{n}\ps{e_l}{x}e_l} \\
&=\ps{e_k}{x}-\ps{e_k}{x} \\
&=0.
\end{aligned}\]

Donc par combinaison linéaire : \(\quantifs{\forall y\in\Vect{e_1,\dots,e_n}}\ps{y}{x-q\paren{x}}=0\).

Donc \(x-q\paren{x}\in F\ortho\).
\end{dem}

\begin{cor}
Soient \(E\) un espace euclidien et \(F\) un sous-espace vectoriel de \(E\).

On a : \[\dim F\ortho=\dim E-\dim F.\]
\end{cor}

\begin{dem}
Comme \(F\) est de dimension finie, on sait (selon le théorème précédent) que \(F\ortho\) est un supplémentaire de \(F\). La formule en découle.
\end{dem}

\begin{defi}
Soient \(E\) un espace préhilbertien, \(F\) un sous-espace vectoriel de \(E\) et \(x\) un vecteur de \(E\).

On appelle distance de \(x\) à \(F\) le réel : \[d\paren{x,F}=\inf_{z\in F}d\paren{x,z}=\inf_{z\in F}\norme{x-z}\in\Rp.\]
\end{defi}

\begin{dem}
Cette borne inférieure est bien définie car \(\accol{d\paren{x,z}}_{z\in F}\) est une partie non-vide de \(\R\) (elle contient \(d\paren{x,0}\)) et minorée (par \(0\)).
\end{dem}

\begin{prop}
Soient \(E\) un espace préhilbertien, \(F\) un sous-espace vectoriel de \(E\) de dimension finie et \(x\) un vecteur de \(E\).

On note \(p_F\) le projecteur orthogonal sur \(F\).

Alors : \[d\paren{x,F}=\norme{x-p_F\paren{x}}.\]

Ainsi, la distance (qui est une borne inférieure) est en fait atteinte.

De plus, \(p_F\paren{x}\) est l'unique élément de \(F\) où la distance de \(x\) à \(F\) est atteinte.

Enfin : \[\norme{x}^2=\norme{p_F\paren{x}}^2+\norme{x-p_F\paren{x}}^2.\]
\end{prop}

\begin{dem}
On a : \[\begin{WithArrows}
\quantifs{\forall z\in F}\norme{x-z}^2&=\norme{\underbrace{x-p_F\paren{x}}_{\in F\ortho}+\underbrace{p_F\paren{x}-z}_{\in F}}^2 \Arrow[tikz={text width=2.5cm}]{selon le théorème de Pythagore} \\
&=\norme{x-p_F\paren{x}}^2+\norme{p_F\paren{x}-z}^2 \\
&\geq\norme{x-p_F\paren{x}}^2\text{ avec égalité ssi }p_F\paren{x}=z.
\end{WithArrows}\]

D'où \(\min_{z\in F}\norme{x-z}=\norme{x-p_F\paren{x}}\).

Enfin, en prenant \(z=0_E\), on a : \(\norme{x}^2=\norme{p_F\paren{x}}^2+\norme{x-p_F\paren{x}}^2\).
\end{dem}

\begin{exoex}
Soient \(E\) un espace euclidien et \(v\in E\) unitaire.

On pose : \[D=\Vect{v}\qquad\text{et}\qquad H=\Vect{v}\ortho.\]

Soit \(x\in E\).

\begin{enumerate}
    \item Donner le projeté orthogonal de \(x\) sur \(D\). \\
    \item Donner le projeté orthogonal de \(x\) sur \(H\). \\
    \item Donner la distance de \(x\) à \(D\). \\
    \item Donner la distance de \(x\) à \(H\).
\end{enumerate}
\end{exoex}

\begin{corr}
On remarque que \(\paren{v}\) est une base orthonormale de \(D\).

On a le projeté orthogonal de \(x\) sur \(D\) : \(p_D\paren{x}=\ps{v}{x}v\).

On a le projeté orthogonal de \(x\) sur \(H\) : \(p_H\paren{x}=x-\ps{v}{x}v\).

On a la distance de \(x\) à \(D\) : \(\norme{x-p_D\paren{x}}=\norme{x-\ps{v}{x}v}\).

On a la distance de \(x\) à \(H\) : \[\begin{WithArrows}
\norme{x-p_H\paren{x}}&=\norme{x-x+\ps{v}{x}v} \\
&=\norme{\ps{v}{x}v} \Arrow{car \(v\) est unitaire} \\
&=\abs{\ps{v}{x}}
\end{WithArrows}\]
\end{corr}

\subsection{Procédé d'orthonormalisation de Gram-Schmidt}

\begin{theo}
Soient \(E\) un espace euclidien et \(\fami{B}=\paren{e_1,\dots,e_n}\) une base de \(E\).

Alors il existe une base orthonormale \(\fami{B}\prim=\paren{e_1\prim,\dots,e_n\prim}\) de \(E\) telle que : \[\quantifs{\forall j\in\interventierii{1}{n}}e_j\prim\in\Vect{e_1,\dots,e_j},\] \cad telle que la matrice de passage \(\pass{\fami{B}}{\fami{B}\prim}\) soit triangulaire supérieure.
\end{theo}

\begin{dem}
On calcule successivement : \[e_1\prim=\dfrac{e_1}{\norme{e_1}}\qquad e_2\prim=\dfrac{e_2-\ps{e_1\prim}{e_2}e_1\prim}{\norme{e_2-\ps{e_1\prim}{e_2}e_1\prim}}\qquad e_3\prim=\dfrac{e_3-\ps{e_1\prim}{e_3}e_1\prim-\ps{e_2\prim}{e_3}e_2\prim}{\norme{e_3-\ps{e_1\prim}{e_3}e_1\prim-\ps{e_2\prim}{e_3}e_2\prim}}\qquad\text{...}\]

Formule générale : \(\quantifs{\forall k\in\interventierii{1}{n}}e_k\prim=\dfrac{e_k-\ds\sum_{j=1}^{k-1}\ps{e_j\prim}{e_k}e_j\prim}{\norme{e_k-\ds\sum_{j=1}^{k-1}\ps{e_j\prim}{e_k}e_j\prim}}\).
\end{dem}

\begin{cor}
Tout espace euclidien admet une base orthonormale.
\end{cor}

\begin{dem}
Soit \(E\) un espace euclidien.

Il existe une base \(\fami{B}\) de \(E\) car \(\dim E<\pinf\).

On en déduit une base orthonormale de \(E\) en appliquant à \(\fami{B}\) l'algorithme de Gram-Schmidt.
\end{dem}

\begin{theo}[Théorème de la base orthonormée incomplète]
Soient \(E\) un espace euclidien et \(\paren{e_1,\dots,e_r}\) une famille orthonormale de vecteurs de \(E\).

Alors on peut compléter \(\paren{e_1,\dots,e_r}\) en une base orthonormale de \(E\).
\end{theo}

\begin{dem}
La famille \(\paren{e_1,\dots,e_r}\) est une famille libre de vecteurs de \(E\).

Selon le théorème de la base incomplète, on peut la compléter en une base \(\paren{e_1,\dots,e_n}\) de \(E\).

En appliquant l'algorithme de Gram-Schmidt à cette base, on obtient une base orthonormale \(\paren{e_1\prim,\dots,e_n\prim}\) de \(E\).

On remarque \(\quantifs{\forall k\in\interventierii{1}{r}}e_k\prim=e_k\) donc on a complété \(\paren{e_1,\dots,e_r}\) en la base orthonormale \[\paren{e_1,\dots,e_r,e_{r+1}\prim,\dots,e_n\prim}.\]
\end{dem}

\begin{exoex}\thlabel{exoex:orthonormalisationEtProjetéOrthogonalEtDistanceDansR4}
On munit \(\R^4\) de son produit scalaire canonique et on pose : \[e_1=\begin{pmatrix}
1 \\ 1 \\ 0 \\ 0
\end{pmatrix}\qquad e_2=\begin{pmatrix}
1 \\ 1 \\ 1 \\ 1
\end{pmatrix}\qquad e_3=\begin{pmatrix}
1 \\ 2 \\ 3 \\ 4
\end{pmatrix}\qquad v=\begin{pmatrix}
1 \\ 0 \\ 0 \\ 0
\end{pmatrix}\]

\begin{enumerate}
    \item Déterminer une base orthonormale de \(F=\Vect{e_1,e_2,e_3}\). \\
    \item Calculer le projeté orthogonal de \(v\) sur \(F\) et la distance de \(v\) à \(F\).
\end{enumerate}
\end{exoex}

\begin{corr}[1]
On sait que \(\paren{e_1,e_2,e_3}\) est une base de \(F\). Orthonormalisons cette base.

On pose : \[e_1\prim=\dfrac{e_1}{\norme{e_1}}\qquad e_2\prim=\dfrac{e_2-\ps{e_1\prim}{e_2}e_1\prim}{\norme{e_2-\ps{e_1\prim}{e_2}e_1\prim}}\qquad e_3\prim=\dfrac{e_3-\ps{e_1\prim}{e_3}e_1\prim-\ps{e_2\prim}{e_3}e_2\prim}{\norme{e_3-\ps{e_1\prim}{e_3}e_1\prim-\ps{e_2\prim}{e_3}e_2\prim}}\]

On a \(\norme{e_1}=\sqrt{2}\) donc \(e_1\prim=\dfrac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ 1 \\ 0 \\ 0
\end{pmatrix}\).

De plus, on a \(\ps{e_1\prim}{e_2}=\ps{\dfrac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ 1 \\ 0 \\ 0
\end{pmatrix}}{\begin{pmatrix}
1 \\ 1 \\ 1 \\ 1
\end{pmatrix}}=\dfrac{2}{\sqrt{2}}=\sqrt{2}\) et \(e_2-\sqrt{2}e_1\prim=\begin{pmatrix}
0 \\ 0 \\ 1 \\ 1
\end{pmatrix}\) et \(\norme{\begin{pmatrix}
0 \\ 0 \\ 1 \\ 1
\end{pmatrix}}=\sqrt{2}\).

Donc \(e_2\prim=\dfrac{1}{\sqrt{2}}\begin{pmatrix}
0 \\ 0 \\ 1 \\ 1
\end{pmatrix}\).

De plus, on a \(\ps{e_1\prim}{e_3}=\ps{\dfrac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ 1 \\ 0 \\ 0
\end{pmatrix}}{\begin{pmatrix}
1 \\ 2 \\ 3 \\ 4
\end{pmatrix}}=\dfrac{3}{\sqrt{2}}\) et \(\ps{e_2\prim}{e_3}=\ps{\dfrac{1}{\sqrt{2}}\begin{pmatrix}
0 \\ 0 \\ 1 \\ 1
\end{pmatrix}}{\begin{pmatrix}
1 \\ 2 \\ 3 \\ 4
\end{pmatrix}}=\dfrac{7}{\sqrt{2}}\) et \(e_3-\dfrac{3}{2}\begin{pmatrix}
1 \\ 1 \\ 0 \\ 0
\end{pmatrix}-\dfrac{7}{2}\begin{pmatrix}
0 \\ 0 \\ 1 \\ 1
\end{pmatrix}=\dfrac{1}{2}\begin{pmatrix}
-1 \\ 1 \\ -1 \\ 1
\end{pmatrix}\) et \(\norme{\dfrac{1}{2}\begin{pmatrix}
-1 \\ 1 \\ -1 \\ 1
\end{pmatrix}}=\dfrac{1}{2}\times2=1\).

Donc \(e_3\prim=\dfrac{1}{2}\begin{pmatrix}
-1 \\ 1 \\ -1 \\ 1
\end{pmatrix}\).

Finalement, \(\paren{e_1\prim,e_2\prim,e_3\prim}\) est une base orthonormale de \(F\).
\end{corr}

\begin{corr}[2]
On a : \[p_F\paren{v}=\ps{e_1\prim}{v}e_1\prim+\ps{e_2\prim}{v}e_2\prim+\ps{e_3\prim}{v}e_3\prim.\]

Or, on a : \[\ps{e_1\prim}{v}=\dfrac{1}{\sqrt{2}}\qquad\ps{e_2\prim}{v}=0\qquad\ps{e_3\prim}{v}=\dfrac{-1}{2}\]

D'où : \[\begin{aligned}
p_F\paren{v}&=\dfrac{1}{\sqrt{2}}\times\dfrac{1}{\sqrt{2}}\begin{pmatrix}
1 \\ 1 \\ 0 \\ 0
\end{pmatrix}-\dfrac{1}{2}\times\dfrac{1}{2}\begin{pmatrix}
-1 \\ 1 \\ -1 \\ 1
\end{pmatrix} \\
&=\dfrac{1}{2}\begin{pmatrix}
1 \\ 1 \\ 0 \\ 0
\end{pmatrix}-\dfrac{1}{4}\begin{pmatrix}
-1 \\ 1 \\ -1 \\ 1
\end{pmatrix} \\
&=\dfrac{1}{4}\begin{pmatrix}
3 \\ 1 \\ 1 \\ -1
\end{pmatrix}
\end{aligned}\]

D'où : \[\begin{aligned}
d\paren{v,F}&=\norme{v-p_F\paren{v}} \\
&=\norme{\begin{pmatrix}
1 \\ 0 \\ 0 \\ 0
\end{pmatrix}-\dfrac{1}{4}\begin{pmatrix}
3 \\ 1 \\ 1 \\ -1
\end{pmatrix}} \\
&=\norme{\dfrac{1}{4}\begin{pmatrix}
1 \\ -1 \\ -1 \\ 1
\end{pmatrix}} \\
&=\dfrac{1}{4}\times2 \\
&=\dfrac{1}{2}.
\end{aligned}\]
\end{corr}

\subsection{Hyperplans d'un espace euclidien}

\begin{defprop}
Soient \(E\) un espace euclidien et \(H\) un hyperplan de \(E\).

On appelle vecteur normal à \(H\) tout vecteur \(v\) non-nul et orthogonal à tout vecteur de \(H\) : \[v\in H\ortho\excluant\accol{0_E}.\]

Il vérifie : \[\quantifs{\forall x\in E}x\in H\ssi v\perp x.\]
\end{defprop}

\begin{dem}
On a \(\dim E<\pinf\) donc \(\dim H\ortho=\dim E-\dim H=1\) donc il existe \(v\in H\ortho\excluant\accol{0_E}\) donc \(H\) admet un vecteur normal.

Posons \(D=\Vect{v}\).

On a \(D=H\ortho\) donc \(H=D\ortho\) selon l'\thref{exo:orthogonalD'OrthogonalEgalALuiMême}.

Donc : \[\begin{aligned}
\quantifs{\forall x\in E}x\in H&\ssi x\in D\ortho \\
&\ssi x\perp v \\
&\ssi\ps{x}{v}=0\qquad\text{(équation cartésienne de \(H\)).}
\end{aligned}\]
\end{dem}

\begin{prop}[Isomorphisme canonique entre un espace euclidien et son dual]
Soient \(E\) un espace euclidien et \(l\in E\etoile\) une forme linéaire sur \(E\).

Il existe un unique vecteur \(v\in E\) tel que : \[\quantifs{\forall x\in E}l\paren{x}=\ps{v}{x}.\]
\end{prop}

\begin{dem}
Posons \(\fonction{\phi}{E}{E\etoile}{v}{\fonctionlambda{E}{\R}{x}{\ps{v}{x}}}\)

On a \(\phi\in\L{E}{E\etoile}\).

Montrons que \(\phi\) est une injection.

Soit \(v\in\ker\phi\).

On a \(\phi\paren{v}=0\) donc \(\quantifs{\forall x\in E}\ps{v}{x}=0\).

Donc \(\ps{v}{v}=0\).

Donc \(v=0\).

Donc \(\ker\phi=\accol{0}\) donc \(\phi\) est une injection.

De plus, \(\dim E=\dim E\etoile<\pinf\) donc \(\phi\) est une surjection.

Donc \(\phi\) est un isomorphisme : \(\quantifs{\forall l\in E\etoile;\exists!v\in E}l=\phi\paren{v}\).

Donc : \[\quantifs{\forall l\in E\etoile;\exists!v\in E;\forall x\in E}l\paren{x}=\ps{v}{x}.\]
\end{dem}

\begin{exoex}
On a vu dans l'\thref{exoex:orthonormalisationEtProjetéOrthogonalEtDistanceDansR4} un hyperplan \(F\) de \(\R^4\).

Donner un vecteur normal à \(F\).
\end{exoex}

\begin{corr}
On a \(v-p_F\paren{v}\in F\ortho\) et \(v-p_F\paren{v}\not=0_{\R^4}\) donc \(v=\dfrac{1}{4}\begin{pmatrix}
1 \\ -1 \\ -1 \\ 1
\end{pmatrix}\) est un vecteur normal à \(F\) et donc \(\begin{pmatrix}
1 \\ -1 \\ -1 \\ 1
\end{pmatrix}\) aussi.

Ainsi : \[\begin{aligned}
\quantifs{\forall\begin{pmatrix}a \\ b \\ c \\ d\end{pmatrix}\in\R^4}\begin{pmatrix}a \\ b \\ c \\ d\end{pmatrix}\in F&\ssi\ps{\begin{pmatrix}a \\ b \\ c \\ d\end{pmatrix}}{\begin{pmatrix}
1 \\ -1 \\ -1 \\ 1
\end{pmatrix}}=0 \\
&\ssi a-b-c+d=0\qquad\text{(équation cartésienne de \(F\)).}
\end{aligned}\]
\end{corr}

\begin{rem}
Soit \(E\) un espace euclidien.

On retrouve que les hyperplans de \(E\) sont les noyaux des formes linéaires non-nulles de \(E\).
\end{rem}

\begin{dem}
Soient \(\fami{B}=\paren{e_1,\dots,e_n}\) une base orthonormale de \(E\), \(H\) un hyperplan de \(E\) et \(v\) un vecteur normal à \(H\) de coordonnées \(\paren{a_1,\dots,a_n}\) dans \(\fami{B}\).

On a : \[\begin{aligned}
\quantifs{\forall\tcoords{x_1}{\vdots}{x_n}\in\R^n}\sum_{k=1}^{n}x_ke_k\in H&\ssi\sum_{k=1}^{n}x_ke_k\perp\sum_{k=1}^{n}a_ke_k \\
&\ssi\sum_{k=1}^{n}a_kx_k=0 \\
&\ssi\tcoords{x_1}{\vdots}{x_n}\in\ker l
\end{aligned}\] avec \(l:\begin{dcases}
e_1\mapsto a_1 \\
\vdots \\
e_n\mapsto a_n
\end{dcases}\)
\end{dem}

\note{À FINIR}